---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: observatorium-loki-stage
spec:
  groups:
  - name: loki_alerts
    rules:
    - alert: LokiRequestErrors
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/no-dashboard/loki_alerts?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace=telemeter-stage&var-job=All&var-pod=All&var-interval=5m
        message: |
          {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}% errors.
        runbook: https://gitlab.cee.redhat.com/observatorium/configuration/blob/master/docs/sop/observatorium.md#lokirequesterrors
      expr: |
        100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route)
          /
        sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route)
          > 10
      for: 15m
      labels:
        service: telemeter
        severity: info
    - alert: LokiRequestPanics
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/no-dashboard/loki_alerts?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace=telemeter-stage&var-job=All&var-pod=All&var-interval=5m
        message: |
          {{ $labels.job }} is experiencing {{ printf "%.2f" $value }}% increase of panics.
        runbook: https://gitlab.cee.redhat.com/observatorium/configuration/blob/master/docs/sop/observatorium.md#lokirequestpanics
      expr: |
        sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
      labels:
        service: telemeter
        severity: info
    - alert: LokiRequestLatency
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/no-dashboard/loki_alerts?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace=telemeter-stage&var-job=All&var-pod=All&var-interval=5m
        message: |
          {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}s 99th percentile latency.
        runbook: https://gitlab.cee.redhat.com/observatorium/configuration/blob/master/docs/sop/observatorium.md#lokirequestlatency
      expr: |
        namespace_job_route:loki_request_duration_seconds:99quantile{route!~"(?i).*tail.*"} > 1
      for: 15m
      labels:
        service: telemeter
        severity: info
