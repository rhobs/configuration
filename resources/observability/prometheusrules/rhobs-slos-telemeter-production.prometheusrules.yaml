---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: app-sre
    role: alert-rules
  name: rhobs-slos-telemeter-production
spec:
  groups:
  - name: rhobs-telemeter-telemeter-server-metrics-write-availability.slo
    rules:
    - alert: TelemeterServerMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload or /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(haproxy_server_http_responses_total:burnrate5m{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate1h{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
        service: telemeter
        severity: critical
    - alert: TelemeterServerMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload or /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(haproxy_server_http_responses_total:burnrate30m{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate6h{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
        service: telemeter
        severity: critical
    - alert: TelemeterServerMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload or /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(haproxy_server_http_responses_total:burnrate2h{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate1d{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
        service: telemeter
        severity: medium
    - alert: TelemeterServerMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-telemeter-server-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload or /receive is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(haproxy_server_http_responses_total:burnrate6h{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
        and
        sum(haproxy_server_http_responses_total:burnrate3d{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$",code="5xx"}[1d]))
        /
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}[1d]))
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate1d
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$",code="5xx"}[1h]))
        /
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}[1h]))
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate1h
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$",code="5xx"}[2h]))
        /
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}[2h]))
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate2h
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$",code="5xx"}[30m]))
        /
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}[30m]))
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate30m
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$",code="5xx"}[3d]))
        /
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}[3d]))
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate3d
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$",code="5xx"}[5m]))
        /
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}[5m]))
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate5m
    - expr: |
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$",code="5xx"}[6h]))
        /
        sum(rate(haproxy_server_http_responses_total{route=~"telemeter-server-upload|telemeter-server-metrics-v1-receive",code=~"^(2..|3..|5..)$"}[6h]))
      labels:
        route: telemeter-server-upload|telemeter-server-metrics-v1-receive
      record: haproxy_server_http_responses_total:burnrate6h
  - name: rhobs-telemeter-telemeter-server-metrics-write-latency.slo
    rules:
    - alert: TelemeterServerMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-telemeter-server-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=telemeter-server,handler=~upload|receive,code=~^(2..|3..|5..)$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricswritelatencyerrorbudgetburning
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1h{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (14.4*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate5m{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate6h{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (6*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate30m{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (6*0.100000)
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
        service: telemeter
        severity: critical
    - alert: TelemeterServerMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-telemeter-server-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=telemeter-server,handler=~upload|receive,code=~^(2..|3..|5..)$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#telemeterservermetricswritelatencyerrorbudgetburning
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1d{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (3*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate2h{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (3*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate3d{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (0.100000)
          and
          latencytarget:http_request_duration_seconds:rate6h{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",latency="5"} > (0.100000)
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[5m]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$"}[5m]))
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[30m]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$"}[30m]))
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[1h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$"}[1h]))
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[2h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$"}[2h]))
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[6h]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$"}[6h]))
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[1d]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$"}[1d]))
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[3d]))
          /
          sum(rate(http_request_duration_seconds_count{job="telemeter-server",handler=~"upload|receive",code=~"^(2..|3..|5..)$"}[3d]))
        )
      labels:
        handler: upload|receive
        job: telemeter-server
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate3d
  - name: rhobs-telemeter-api-metrics-write-availability.slo
    rules:
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: critical
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: critical
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-write-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswriteavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        handler: receive
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}[1d]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}[1h]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}[2h]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}[30m]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}[3d]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}[5m]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler=~"receive",code=~"^(2..|3..|5..)$"}[6h]))
      labels:
        handler: receive
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
  - name: rhobs-telemeter-api-metrics-write-latency.slo
    rules:
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=observatorium-observatorium-api,handler=receive,code=~^(2..|3..|5..)$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswritelatencyerrorbudgetburning
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1h{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (14.4*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate5m{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate6h{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (6*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate30m{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (6*0.100000)
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
        service: telemeter
        severity: critical
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-write-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for job=observatorium-observatorium-api,handler=receive,code=~^(2..|3..|5..)$,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricswritelatencyerrorbudgetburning
      expr: |
        (
          latencytarget:http_request_duration_seconds:rate1d{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (3*0.100000)
          and
          latencytarget:http_request_duration_seconds:rate2h{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (3*0.100000)
        )
        or
        (
          latencytarget:http_request_duration_seconds:rate3d{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (0.100000)
          and
          latencytarget:http_request_duration_seconds:rate6h{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",latency="5"} > (0.100000)
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[5m]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$"}[5m]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[30m]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$"}[30m]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[1h]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$"}[1h]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[2h]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$"}[2h]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[6h]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$"}[6h]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[1d]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$"}[1d]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(http_request_duration_seconds_bucket{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$",le="5",code!~"5.."}[3d]))
          /
          sum(rate(http_request_duration_seconds_count{job="observatorium-observatorium-api",handler="receive",code=~"^(2..|3..|5..)$"}[3d]))
        )
      labels:
        handler: receive
        job: observatorium-observatorium-api
        latency: "5"
      record: latencytarget:http_request_duration_seconds:rate3d
  - name: rhobs-telemeter-api-metrics-read-availability.slo
    rules:
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: critical
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: critical
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        handler: query
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}[1d]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}[1h]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}[2h]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}[30m]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}[3d]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}[5m]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query",code=~"^(2..|3..|5..)$"}[6h]))
      labels:
        handler: query
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate5m{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1h{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (14.40 * (1-0.95000))
      for: 2m
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: critical
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate30m{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (6.00 * (1-0.95000))
      for: 15m
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: critical
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate2h{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate1d{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (3.00 * (1-0.95000))
      for: 1h
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - alert: APIMetricsReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-availability.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee availability SLOs
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadavailabilityerrorbudgetburning
      expr: |
        sum(http_requests_total:burnrate6h{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
        and
        sum(http_requests_total:burnrate3d{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}) > (1.00 * (1-0.95000))
      for: 3h
      labels:
        handler: query_range
        job: observatorium-observatorium-api
        service: telemeter
        severity: medium
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$",code=~"5.+"}[1d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}[1d]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$",code=~"5.+"}[1h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}[1h]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate1h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$",code=~"5.+"}[2h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}[2h]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate2h
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$",code=~"5.+"}[30m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}[30m]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate30m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$",code=~"5.+"}[3d]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}[3d]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate3d
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$",code=~"5.+"}[5m]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}[5m]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate5m
    - expr: |
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$",code=~"5.+"}[6h]))
        /
        sum(rate(http_requests_total{job="observatorium-observatorium-api",handler="query_range",code=~"^(2..|3..|5..)$"}[6h]))
      labels:
        handler: query_range
        job: observatorium-observatorium-api
      record: http_requests_total:burnrate6h
  - name: rhobs-telemeter-api-metrics-read-latency.slo
    rules:
    - alert: APIMetricsReadLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-1M-samples,namespace=observatorium-production,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1h{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (14.4*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate5m{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (6*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate30m{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (6*0.100000)
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
        service: telemeter
        severity: critical
    - alert: APIMetricsReadLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-1M-samples,namespace=observatorium-production,latency=5 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1d{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (3*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate2h{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (3*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate3d{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-1M-samples",namespace="observatorium-production",latency="5"} > (0.100000)
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-production",le="5",code!~"5.."}[5m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-production"}[5m]))
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-production",le="5",code!~"5.."}[30m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-production"}[30m]))
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-production",le="5",code!~"5.."}[1h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-production"}[1h]))
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-production",le="5",code!~"5.."}[2h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-production"}[2h]))
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-production",le="5",code!~"5.."}[6h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-production"}[6h]))
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-production",le="5",code!~"5.."}[1d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-production"}[1d]))
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-1M-samples",namespace="observatorium-production",le="5",code!~"5.."}[3d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-1M-samples",namespace="observatorium-production"}[3d]))
        )
      labels:
        latency: "5"
        namespace: observatorium-production
        query: query-path-sli-1M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate3d
    - alert: APIMetricsReadLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-10M-samples,namespace=observatorium-production,latency=15 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1h{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (14.4*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate5m{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (6*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate30m{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (6*0.100000)
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
        service: telemeter
        severity: critical
    - alert: APIMetricsReadLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-10M-samples,namespace=observatorium-production,latency=15 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1d{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (3*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate2h{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (3*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate3d{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-10M-samples",namespace="observatorium-production",latency="15"} > (0.100000)
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-production",le="15",code!~"5.."}[5m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-production"}[5m]))
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-production",le="15",code!~"5.."}[30m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-production"}[30m]))
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-production",le="15",code!~"5.."}[1h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-production"}[1h]))
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-production",le="15",code!~"5.."}[2h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-production"}[2h]))
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-production",le="15",code!~"5.."}[6h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-production"}[6h]))
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-production",le="15",code!~"5.."}[1d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-production"}[1d]))
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-10M-samples",namespace="observatorium-production",le="15",code!~"5.."}[3d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-10M-samples",namespace="observatorium-production"}[3d]))
        )
      labels:
        latency: "15"
        namespace: observatorium-production
        query: query-path-sli-10M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate3d
    - alert: APIMetricsReadLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-100M-samples,namespace=observatorium-production,latency=100 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1h{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (14.4*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate5m{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (14.4*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (6*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate30m{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (6*0.100000)
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
        service: telemeter
        severity: critical
    - alert: APIMetricsReadLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/f9fa7677fb4a2669f123f9a0f2234b47/rhobs-telemeter-api-metrics-read-latency.slo?orgId=1&refresh=10s&var-datasource=telemeter-prod-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: 'High requests latency budget burn for query=query-path-sli-100M-samples,namespace=observatorium-production,latency=100 (current value: {{ $value }})'
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#apimetricsreadlatencyerrorbudgetburning
      expr: |
        (
          latencytarget:up_custom_query_duration_seconds:rate1d{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (3*0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate2h{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (3*0.100000)
        )
        or
        (
          latencytarget:up_custom_query_duration_seconds:rate3d{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (0.100000)
          and
          latencytarget:up_custom_query_duration_seconds:rate6h{query="query-path-sli-100M-samples",namespace="observatorium-production",latency="100"} > (0.100000)
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
        service: telemeter
        severity: medium
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-production",le="100",code!~"5.."}[5m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-production"}[5m]))
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate5m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-production",le="100",code!~"5.."}[30m]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-production"}[30m]))
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate30m
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-production",le="100",code!~"5.."}[1h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-production"}[1h]))
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-production",le="100",code!~"5.."}[2h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-production"}[2h]))
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate2h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-production",le="100",code!~"5.."}[6h]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-production"}[6h]))
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate6h
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-production",le="100",code!~"5.."}[1d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-production"}[1d]))
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate1d
    - expr: |
        1 - (
          sum(rate(up_custom_query_duration_seconds_bucket{query="query-path-sli-100M-samples",namespace="observatorium-production",le="100",code!~"5.."}[3d]))
          /
          sum(rate(up_custom_query_duration_seconds_count{query="query-path-sli-100M-samples",namespace="observatorium-production"}[3d]))
        )
      labels:
        latency: "100"
        namespace: observatorium-production
        query: query-path-sli-100M-samples
      record: latencytarget:up_custom_query_duration_seconds:rate3d
