---
$schema: /openshift/prometheus-rule-1.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    prometheus: app-sre
    role: alert-rules
  name: rhobs-slos-telemeter-stage
spec:
  groups:
  - interval: 2m30s
    name: rhobs-telemeter-server-metrics-upload-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[4w]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      expr: absent(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"})
        == 1
      for: 2m
      labels:
        route: telemeter-server-upload
        severity: high
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-upload-availability-slo
    rules:
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}[5m]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[5m]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:burnrate5m
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}[30m]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[30m]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:burnrate30m
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}[1h]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[1h]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:burnrate1h
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}[2h]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[2h]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:burnrate2h
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}[6h]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[6h]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:burnrate6h
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}[1d]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[1d]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:burnrate1d
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}[4d]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"}[4d]))
      labels:
        route: telemeter-server-upload
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: haproxy_server_http_responses:burnrate4d
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate5m{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (14 * (1-0.995)) and haproxy_server_http_responses:burnrate1h{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        long: 1h
        route: telemeter-server-upload
        severity: high
        short: 5m
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate30m{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (7 * (1-0.995)) and haproxy_server_http_responses:burnrate6h{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        long: 6h
        route: telemeter-server-upload
        severity: high
        short: 30m
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate2h{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (2 * (1-0.995)) and haproxy_server_http_responses:burnrate1d{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        long: 1d
        route: telemeter-server-upload
        severity: warning
        short: 2h
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
    - alert: TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate6h{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (1 * (1-0.995)) and haproxy_server_http_responses:burnrate4d{code!~"^4..$",route="telemeter-server-upload",slo="rhobs-telemeter-server-metrics-upload-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        long: 4d
        route: telemeter-server-upload
        severity: warning
        short: 6h
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-upload-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: pyrra_window
    - expr: 1 - sum(haproxy_server_http_responses:increase4w{code=~"5..",route="telemeter-server-upload"}
        or vector(0)) / sum(haproxy_server_http_responses:increase4w{code!~"^4..$",route="telemeter-server-upload"})
      labels:
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: pyrra_availability
    - expr: sum(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-upload"})
      labels:
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: pyrra_requests_total
    - expr: sum(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-upload"}
        or vector(0))
      labels:
        slo: rhobs-telemeter-server-metrics-upload-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: rhobs-telemeter-server-metrics-receive-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[4w]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      expr: absent(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"})
        == 1
      for: 2m
      labels:
        route: telemeter-server-metrics-v1-receive
        severity: high
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-receive-availability-slo
    rules:
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}[5m]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[5m]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:burnrate5m
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}[30m]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[30m]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:burnrate30m
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}[1h]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[1h]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:burnrate1h
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}[2h]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[2h]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:burnrate2h
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}[6h]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[6h]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:burnrate6h
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}[1d]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[1d]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:burnrate1d
    - expr: sum by(code) (rate(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}[4d]))
        / sum by(code) (rate(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"}[4d]))
      labels:
        route: telemeter-server-metrics-v1-receive
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: haproxy_server_http_responses:burnrate4d
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate5m{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (14 * (1-0.995)) and haproxy_server_http_responses:burnrate1h{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        long: 1h
        route: telemeter-server-metrics-v1-receive
        severity: high
        short: 5m
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate30m{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (7 * (1-0.995)) and haproxy_server_http_responses:burnrate6h{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        long: 6h
        route: telemeter-server-metrics-v1-receive
        severity: high
        short: 30m
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate2h{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (2 * (1-0.995)) and haproxy_server_http_responses:burnrate1d{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        long: 1d
        route: telemeter-server-metrics-v1-receive
        severity: warning
        short: 2h
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
    - alert: TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteAvailabilityErrorBudgetBurning
      expr: haproxy_server_http_responses:burnrate6h{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (1 * (1-0.995)) and haproxy_server_http_responses:burnrate4d{code!~"^4..$",route="telemeter-server-metrics-v1-receive",slo="rhobs-telemeter-server-metrics-receive-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        long: 4d
        route: telemeter-server-metrics-v1-receive
        severity: warning
        short: 6h
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-receive-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: pyrra_window
    - expr: 1 - sum(haproxy_server_http_responses:increase4w{code=~"5..",route="telemeter-server-metrics-v1-receive"}
        or vector(0)) / sum(haproxy_server_http_responses:increase4w{code!~"^4..$",route="telemeter-server-metrics-v1-receive"})
      labels:
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: pyrra_availability
    - expr: sum(haproxy_server_http_responses_total{code!~"^4..$",route="telemeter-server-metrics-v1-receive"})
      labels:
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: pyrra_requests_total
    - expr: sum(haproxy_server_http_responses_total{code=~"5..",route="telemeter-server-metrics-v1-receive"}
        or vector(0))
      labels:
        slo: rhobs-telemeter-server-metrics-receive-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: rhobs-telemeter-server-metrics-upload-latency-slo-increase
    rules:
    - expr: sum by(code) (increase(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[4w]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:increase4w
    - expr: sum by(code) (increase(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[4w]))
      labels:
        handler: upload
        job: telemeter-server
        le: "5"
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"})
        == 1
      for: 2m
      labels:
        handler: upload
        job: telemeter-server
        severity: high
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"})
        == 1
      for: 2m
      labels:
        handler: upload
        job: telemeter-server
        le: "5"
        severity: high
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-upload-latency-slo
    rules:
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[5m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[5m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[5m]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:burnrate5m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[30m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[30m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[30m]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:burnrate30m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[1h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[1h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[1h]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:burnrate1h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[2h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[2h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[2h]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:burnrate2h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[6h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[6h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[6h]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:burnrate6h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[1d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[1d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[1d]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:burnrate1d
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[4d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"}[4d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"}[4d]))
      labels:
        handler: upload
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: http_request_duration_seconds:burnrate4d
    - alert: TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate5m{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (14 * (1-0.9)) and http_request_duration_seconds:burnrate1h{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (14 * (1-0.9))
      for: 2m
      labels:
        handler: upload
        job: telemeter-server
        long: 1h
        severity: high
        short: 5m
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
    - alert: TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate30m{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (7 * (1-0.9)) and http_request_duration_seconds:burnrate6h{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (7 * (1-0.9))
      for: 15m
      labels:
        handler: upload
        job: telemeter-server
        long: 6h
        severity: high
        short: 30m
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
    - alert: TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate2h{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (2 * (1-0.9)) and http_request_duration_seconds:burnrate1d{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (2 * (1-0.9))
      for: 1h
      labels:
        handler: upload
        job: telemeter-server
        long: 1d
        severity: warning
        short: 2h
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
    - alert: TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /upload is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsUploadWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate6h{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (1 * (1-0.9)) and http_request_duration_seconds:burnrate4d{handler="upload",job="telemeter-server",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        > (1 * (1-0.9))
      for: 3h
      labels:
        handler: upload
        job: telemeter-server
        long: 4d
        severity: warning
        short: 6h
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-upload-latency-slo-generic
    rules:
    - expr: "0.9"
      labels:
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: pyrra_window
    - expr: sum(http_request_duration_seconds:increase4w{code=~"^2..$",handler="upload",job="telemeter-server",le="5",slo="rhobs-telemeter-server-metrics-upload-latency-slo"}
        or vector(0)) / sum(http_request_duration_seconds:increase4w{code=~"^2..$",handler="upload",job="telemeter-server",le="",slo="rhobs-telemeter-server-metrics-upload-latency-slo"})
      labels:
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: pyrra_availability
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"})
      labels:
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: pyrra_requests_total
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",handler="upload",job="telemeter-server"})
        - sum(http_request_duration_seconds_bucket{code=~"^2..$",handler="upload",job="telemeter-server",le="5"})
      labels:
        slo: rhobs-telemeter-server-metrics-upload-latency-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: rhobs-telemeter-server-metrics-receive-latency-slo-increase
    rules:
    - expr: sum by(code) (increase(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[4w]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:increase4w
    - expr: sum by(code) (increase(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[4w]))
      labels:
        handler: receive
        job: telemeter-server
        le: "5"
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"})
        == 1
      for: 2m
      labels:
        handler: receive
        job: telemeter-server
        severity: high
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"})
        == 1
      for: 2m
      labels:
        handler: receive
        job: telemeter-server
        le: "5"
        severity: high
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-receive-latency-slo
    rules:
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[5m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[5m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[5m]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:burnrate5m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[30m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[30m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[30m]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:burnrate30m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[1h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[1h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[1h]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:burnrate1h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[2h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[2h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[2h]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:burnrate2h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[6h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[6h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[6h]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:burnrate6h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[1d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[1d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[1d]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:burnrate1d
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[4d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"}[4d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"}[4d]))
      labels:
        handler: receive
        job: telemeter-server
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: http_request_duration_seconds:burnrate4d
    - alert: TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate5m{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (14 * (1-0.9)) and http_request_duration_seconds:burnrate1h{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (14 * (1-0.9))
      for: 2m
      labels:
        handler: receive
        job: telemeter-server
        long: 1h
        severity: high
        short: 5m
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
    - alert: TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate30m{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (7 * (1-0.9)) and http_request_duration_seconds:burnrate6h{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (7 * (1-0.9))
      for: 15m
      labels:
        handler: receive
        job: telemeter-server
        long: 6h
        severity: high
        short: 30m
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
    - alert: TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate2h{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (2 * (1-0.9)) and http_request_duration_seconds:burnrate1d{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (2 * (1-0.9))
      for: 1h
      labels:
        handler: receive
        job: telemeter-server
        long: 1d
        severity: warning
        short: 2h
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
    - alert: TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Telemeter Server /receive is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#TelemeterServerMetricsReceiveWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate6h{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (1 * (1-0.9)) and http_request_duration_seconds:burnrate4d{handler="receive",job="telemeter-server",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        > (1 * (1-0.9))
      for: 3h
      labels:
        handler: receive
        job: telemeter-server
        long: 4d
        severity: warning
        short: 6h
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
  - interval: 30s
    name: rhobs-telemeter-server-metrics-receive-latency-slo-generic
    rules:
    - expr: "0.9"
      labels:
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: pyrra_window
    - expr: sum(http_request_duration_seconds:increase4w{code=~"^2..$",handler="receive",job="telemeter-server",le="5",slo="rhobs-telemeter-server-metrics-receive-latency-slo"}
        or vector(0)) / sum(http_request_duration_seconds:increase4w{code=~"^2..$",handler="receive",job="telemeter-server",le="",slo="rhobs-telemeter-server-metrics-receive-latency-slo"})
      labels:
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: pyrra_availability
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"})
      labels:
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: pyrra_requests_total
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",handler="receive",job="telemeter-server"})
        - sum(http_request_duration_seconds_bucket{code=~"^2..$",handler="receive",job="telemeter-server",le="5"})
      labels:
        slo: rhobs-telemeter-server-metrics-receive-latency-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-write-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        severity: high
        slo: api-metrics-write-availability-slo
  - interval: 30s
    name: api-metrics-write-availability-slo
    rules:
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[5m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate5m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[30m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate30m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate1h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[2h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate2h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[6h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate6h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate1d
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[4d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-availability-slo
      record: http_requests:burnrate4d
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (14 * (1-0.995)) and http_requests:burnrate1h{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 1h
        severity: high
        short: 5m
        slo: api-metrics-write-availability-slo
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (7 * (1-0.995)) and http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 6h
        severity: high
        short: 30m
        slo: api-metrics-write-availability-slo
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (2 * (1-0.995)) and http_requests:burnrate1d{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 1d
        severity: warning
        short: 2h
        slo: api-metrics-write-availability-slo
    - alert: APIMetricsWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (1 * (1-0.995)) and http_requests:burnrate4d{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 4d
        severity: warning
        short: 6h
        slo: api-metrics-write-availability-slo
  - interval: 30s
    name: api-metrics-write-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-metrics-write-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-metrics-write-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}
        or vector(0)) / sum(http_requests:increase4w{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"})
      labels:
        slo: api-metrics-write-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{code!~"^4..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"})
      labels:
        slo: api-metrics-write-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}
        or vector(0))
      labels:
        slo: api-metrics-write-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-query-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        severity: high
        slo: api-metrics-query-availability-slo
  - interval: 30s
    name: api-metrics-query-availability-slo
    rules:
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[5m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate5m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[30m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate30m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[1h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate1h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[2h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate2h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[6h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate6h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[1d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate1d
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[4d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        slo: api-metrics-query-availability-slo
      record: http_requests:burnrate4d
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (14 * (1-0.995)) and http_requests:burnrate1h{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        long: 1h
        severity: high
        short: 5m
        slo: api-metrics-query-availability-slo
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (7 * (1-0.995)) and http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        long: 6h
        severity: high
        short: 30m
        slo: api-metrics-query-availability-slo
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (2 * (1-0.995)) and http_requests:burnrate1d{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        long: 1d
        severity: warning
        short: 2h
        slo: api-metrics-query-availability-slo
    - alert: APIMetricsQueryAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (1 * (1-0.995)) and http_requests:burnrate4d{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api",slo="api-metrics-query-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        group: metricsv1
        handler: query
        job: observatorium-observatorium-api
        long: 4d
        severity: warning
        short: 6h
        slo: api-metrics-query-availability-slo
  - interval: 30s
    name: api-metrics-query-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-metrics-query-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-metrics-query-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}
        or vector(0)) / sum(http_requests:increase4w{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"})
      labels:
        slo: api-metrics-query-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{code!~"^4..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"})
      labels:
        slo: api-metrics-query-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="query",job="observatorium-observatorium-api"}
        or vector(0))
      labels:
        slo: api-metrics-query-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-query-range-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        severity: high
        slo: api-metrics-query-range-availability-slo
  - interval: 30s
    name: api-metrics-query-range-availability-slo
    rules:
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[5m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate5m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[30m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate30m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[1h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate1h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[2h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate2h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[6h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate6h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[1d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate1d
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[4d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        slo: api-metrics-query-range-availability-slo
      record: http_requests:burnrate4d
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (14 * (1-0.995)) and http_requests:burnrate1h{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        long: 1h
        severity: high
        short: 5m
        slo: api-metrics-query-range-availability-slo
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (7 * (1-0.995)) and http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        long: 6h
        severity: high
        short: 30m
        slo: api-metrics-query-range-availability-slo
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (2 * (1-0.995)) and http_requests:burnrate1d{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        long: 1d
        severity: warning
        short: 2h
        slo: api-metrics-query-range-availability-slo
    - alert: APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query_range handler is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsQueryRangeAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (1 * (1-0.995)) and http_requests:burnrate4d{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api",slo="api-metrics-query-range-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        group: metricsv1
        handler: query_range
        job: observatorium-observatorium-api
        long: 4d
        severity: warning
        short: 6h
        slo: api-metrics-query-range-availability-slo
  - interval: 30s
    name: api-metrics-query-range-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-metrics-query-range-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-metrics-query-range-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}
        or vector(0)) / sum(http_requests:increase4w{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"})
      labels:
        slo: api-metrics-query-range-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{code!~"^4..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"})
      labels:
        slo: api-metrics-query-range-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="query_range",job="observatorium-observatorium-api"}
        or vector(0))
      labels:
        slo: api-metrics-query-range-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-rules-raw-write-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[4w]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for writes is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawWriteAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        severity: high
        slo: api-rules-raw-write-availability-slo
  - interval: 30s
    name: api-rules-raw-write-availability-slo
    rules:
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[5m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[5m]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:burnrate5m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[30m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[30m]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:burnrate30m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[1h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[1h]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:burnrate1h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[2h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[2h]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:burnrate2h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[6h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[6h]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:burnrate6h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[1d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[1d]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:burnrate1d
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[4d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}[4d]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: PUT
        slo: api-rules-raw-write-availability-slo
      record: http_requests:burnrate4d
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for writes is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (14 * (1-0.995)) and http_requests:burnrate1h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 1h
        method: PUT
        severity: high
        short: 5m
        slo: api-rules-raw-write-availability-slo
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for writes is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (7 * (1-0.995)) and http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 6h
        method: PUT
        severity: high
        short: 30m
        slo: api-rules-raw-write-availability-slo
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for writes is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (2 * (1-0.995)) and http_requests:burnrate1d{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 1d
        method: PUT
        severity: warning
        short: 2h
        slo: api-rules-raw-write-availability-slo
    - alert: APIRulesRawWriteAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for writes is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawWriteAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (1 * (1-0.995)) and http_requests:burnrate4d{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT",slo="api-rules-raw-write-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 4d
        method: PUT
        severity: warning
        short: 6h
        slo: api-rules-raw-write-availability-slo
  - interval: 30s
    name: api-rules-raw-write-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-rules-raw-write-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-rules-raw-write-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}
        or vector(0)) / sum(http_requests:increase4w{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"})
      labels:
        slo: api-rules-raw-write-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"})
      labels:
        slo: api-rules-raw-write-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="PUT"}
        or vector(0))
      labels:
        slo: api-rules-raw-write-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-rules-raw-read-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[4w]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for reads is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawReadAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        severity: high
        slo: api-rules-raw-read-availability-slo
  - interval: 30s
    name: api-rules-raw-read-availability-slo
    rules:
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[5m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[5m]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:burnrate5m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[30m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[30m]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:burnrate30m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[1h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[1h]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:burnrate1h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[2h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[2h]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:burnrate2h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[6h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[6h]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:burnrate6h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[1d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[1d]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:burnrate1d
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[4d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}[4d]))
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-raw-read-availability-slo
      record: http_requests:burnrate4d
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for reads is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (14 * (1-0.995)) and http_requests:burnrate1h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 1h
        method: GET
        severity: high
        short: 5m
        slo: api-rules-raw-read-availability-slo
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for reads is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (7 * (1-0.995)) and http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 6h
        method: GET
        severity: high
        short: 30m
        slo: api-rules-raw-read-availability-slo
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for reads is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (2 * (1-0.995)) and http_requests:burnrate1d{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 1d
        method: GET
        severity: warning
        short: 2h
        slo: api-rules-raw-read-availability-slo
    - alert: APIRulesRawReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules/raw endpoint for reads is burning too much error budget
          to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesRawReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (1 * (1-0.995)) and http_requests:burnrate4d{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET",slo="api-rules-raw-read-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        group: metricsv1
        handler: rules-raw
        job: observatorium-observatorium-api
        long: 4d
        method: GET
        severity: warning
        short: 6h
        slo: api-rules-raw-read-availability-slo
  - interval: 30s
    name: api-rules-raw-read-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-rules-raw-read-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-rules-raw-read-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}
        or vector(0)) / sum(http_requests:increase4w{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"})
      labels:
        slo: api-rules-raw-read-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"})
      labels:
        slo: api-rules-raw-read-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules-raw",job="observatorium-observatorium-api",method="GET"}
        or vector(0))
      labels:
        slo: api-rules-raw-read-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-rules-read-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[4w]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesReadAvailabilityErrorBudgetBurning
      expr: absent(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        severity: high
        slo: api-rules-read-availability-slo
  - interval: 30s
    name: api-rules-read-availability-slo
    rules:
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[5m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[5m]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:burnrate5m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[30m]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[30m]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:burnrate30m
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[1h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[1h]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:burnrate1h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[2h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[2h]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:burnrate2h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[6h]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[6h]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:burnrate6h
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[1d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[1d]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:burnrate1d
    - expr: sum by(code) (rate(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[4d]))
        / sum by(code) (rate(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}[4d]))
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        method: GET
        slo: api-rules-read-availability-slo
      record: http_requests:burnrate4d
    - alert: APIRulesReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate5m{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (14 * (1-0.995)) and http_requests:burnrate1h{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        long: 1h
        method: GET
        severity: high
        short: 5m
        slo: api-rules-read-availability-slo
    - alert: APIRulesReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate30m{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (7 * (1-0.995)) and http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        long: 6h
        method: GET
        severity: high
        short: 30m
        slo: api-rules-read-availability-slo
    - alert: APIRulesReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate2h{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (2 * (1-0.995)) and http_requests:burnrate1d{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        long: 1d
        method: GET
        severity: warning
        short: 2h
        slo: api-rules-read-availability-slo
    - alert: APIRulesReadAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /rules endpoint is burning too much error budget to guarantee
          availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesReadAvailabilityErrorBudgetBurning
      expr: http_requests:burnrate6h{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (1 * (1-0.995)) and http_requests:burnrate4d{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET",slo="api-rules-read-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        group: metricsv1
        handler: rules
        job: observatorium-observatorium-api
        long: 4d
        method: GET
        severity: warning
        short: 6h
        slo: api-rules-read-availability-slo
  - interval: 30s
    name: api-rules-read-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-rules-read-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-rules-read-availability-slo
      record: pyrra_window
    - expr: 1 - sum(http_requests:increase4w{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}
        or vector(0)) / sum(http_requests:increase4w{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"})
      labels:
        slo: api-rules-read-availability-slo
      record: pyrra_availability
    - expr: sum(http_requests_total{code!~"^4..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"})
      labels:
        slo: api-rules-read-availability-slo
      record: pyrra_requests_total
    - expr: sum(http_requests_total{code=~"^5..$",group="metricsv1",handler="rules",job="observatorium-observatorium-api",method="GET"}
        or vector(0))
      labels:
        slo: api-rules-read-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-rules-sync-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[4w]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Thanos Ruler /reload endpoint is burning too much error budget to
          guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesSyncAvailabilityErrorBudgetBurning
      expr: absent(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"})
        == 1
      for: 2m
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        severity: high
        slo: api-rules-sync-availability-slo
  - interval: 30s
    name: api-rules-sync-availability-slo
    rules:
    - expr: sum by(code) (rate(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[5m]))
        / sum by(code) (rate(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[5m]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:burnrate5m
    - expr: sum by(code) (rate(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[30m]))
        / sum by(code) (rate(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[30m]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:burnrate30m
    - expr: sum by(code) (rate(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[1h]))
        / sum by(code) (rate(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[1h]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:burnrate1h
    - expr: sum by(code) (rate(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[2h]))
        / sum by(code) (rate(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[2h]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:burnrate2h
    - expr: sum by(code) (rate(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[6h]))
        / sum by(code) (rate(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[6h]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:burnrate6h
    - expr: sum by(code) (rate(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[1d]))
        / sum by(code) (rate(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[1d]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:burnrate1d
    - expr: sum by(code) (rate(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[4d]))
        / sum by(code) (rate(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}[4d]))
      labels:
        client: reload
        container: thanos-rule-syncer
        namespace: observatorium-metrics-stage
        slo: api-rules-sync-availability-slo
      record: client_api_requests:burnrate4d
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Thanos Ruler /reload endpoint is burning too much error budget to
          guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesSyncAvailabilityErrorBudgetBurning
      expr: client_api_requests:burnrate5m{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (14 * (1-0.995)) and client_api_requests:burnrate1h{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        client: reload
        container: thanos-rule-syncer
        long: 1h
        namespace: observatorium-metrics-stage
        severity: high
        short: 5m
        slo: api-rules-sync-availability-slo
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Thanos Ruler /reload endpoint is burning too much error budget to
          guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesSyncAvailabilityErrorBudgetBurning
      expr: client_api_requests:burnrate30m{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (7 * (1-0.995)) and client_api_requests:burnrate6h{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        client: reload
        container: thanos-rule-syncer
        long: 6h
        namespace: observatorium-metrics-stage
        severity: high
        short: 30m
        slo: api-rules-sync-availability-slo
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Thanos Ruler /reload endpoint is burning too much error budget to
          guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesSyncAvailabilityErrorBudgetBurning
      expr: client_api_requests:burnrate2h{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (2 * (1-0.995)) and client_api_requests:burnrate1d{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        client: reload
        container: thanos-rule-syncer
        long: 1d
        namespace: observatorium-metrics-stage
        severity: warning
        short: 2h
        slo: api-rules-sync-availability-slo
    - alert: APIRulesSyncAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: Thanos Ruler /reload endpoint is burning too much error budget to
          guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIRulesSyncAvailabilityErrorBudgetBurning
      expr: client_api_requests:burnrate6h{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (1 * (1-0.995)) and client_api_requests:burnrate4d{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage",slo="api-rules-sync-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        client: reload
        container: thanos-rule-syncer
        long: 4d
        namespace: observatorium-metrics-stage
        severity: warning
        short: 6h
        slo: api-rules-sync-availability-slo
  - interval: 30s
    name: api-rules-sync-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-rules-sync-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-rules-sync-availability-slo
      record: pyrra_window
    - expr: 1 - sum(client_api_requests:increase4w{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}
        or vector(0)) / sum(client_api_requests:increase4w{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"})
      labels:
        slo: api-rules-sync-availability-slo
      record: pyrra_availability
    - expr: sum(client_api_requests_total{client="reload",code!~"^4..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"})
      labels:
        slo: api-rules-sync-availability-slo
      record: pyrra_requests_total
    - expr: sum(client_api_requests_total{client="reload",code=~"^5..$",container="thanos-rule-syncer",namespace="observatorium-metrics-stage"}
        or vector(0))
      labels:
        slo: api-rules-sync-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-alerting-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[4w]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning
          too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: absent(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"})
        == 1
      for: 2m
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        severity: high
        slo: api-alerting-availability-slo
  - interval: 30s
    name: api-alerting-availability-slo
    rules:
    - expr: sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[5m]))
        / sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[5m]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate5m
    - expr: sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[30m]))
        / sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[30m]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate30m
    - expr: sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[1h]))
        / sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[1h]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate1h
    - expr: sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[2h]))
        / sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[2h]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate2h
    - expr: sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[6h]))
        / sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[6h]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate6h
    - expr: sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[1d]))
        / sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[1d]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate1d
    - expr: sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[4d]))
        / sum by(code) (rate(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"}[4d]))
      labels:
        container: thanos-rule
        namespace: observatorium-metrics-stage
        slo: api-alerting-availability-slo
      record: thanos_alert_sender_alerts_dropped:burnrate4d
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning
          too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate5m{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (14 * (1-0.995)) and thanos_alert_sender_alerts_dropped:burnrate1h{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        container: thanos-rule
        long: 1h
        namespace: observatorium-metrics-stage
        severity: high
        short: 5m
        slo: api-alerting-availability-slo
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning
          too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate30m{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (7 * (1-0.995)) and thanos_alert_sender_alerts_dropped:burnrate6h{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        container: thanos-rule
        long: 6h
        namespace: observatorium-metrics-stage
        severity: high
        short: 30m
        slo: api-alerting-availability-slo
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning
          too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate2h{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (2 * (1-0.995)) and thanos_alert_sender_alerts_dropped:burnrate1d{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        container: thanos-rule
        long: 1d
        namespace: observatorium-metrics-stage
        severity: warning
        short: 2h
        slo: api-alerting-availability-slo
    - alert: APIAlertmanagerAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Thanos Rule failing to send alerts to Alertmanager and is burning
          too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerAvailabilityErrorBudgetBurning
      expr: thanos_alert_sender_alerts_dropped:burnrate6h{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (1 * (1-0.995)) and thanos_alert_sender_alerts_dropped:burnrate4d{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage",slo="api-alerting-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        container: thanos-rule
        long: 4d
        namespace: observatorium-metrics-stage
        severity: warning
        short: 6h
        slo: api-alerting-availability-slo
  - interval: 30s
    name: api-alerting-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-alerting-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-alerting-availability-slo
      record: pyrra_window
    - expr: 1 - sum(thanos_alert_sender_alerts_dropped:increase4w{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}
        or vector(0)) / sum(thanos_alert_sender_alerts_dropped:increase4w{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"})
      labels:
        slo: api-alerting-availability-slo
      record: pyrra_availability
    - expr: sum(thanos_alert_sender_alerts_dropped_total{code!~"^4..$",container="thanos-rule",namespace="observatorium-metrics-stage"})
      labels:
        slo: api-alerting-availability-slo
      record: pyrra_requests_total
    - expr: sum(thanos_alert_sender_alerts_dropped_total{code=~"^5..$",container="thanos-rule",namespace="observatorium-metrics-stage"}
        or vector(0))
      labels:
        slo: api-alerting-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-alerting-notif-availability-slo-increase
    rules:
    - expr: sum by(code) (increase(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[4w]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and
          is burning too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: absent(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"})
        == 1
      for: 2m
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        severity: high
        slo: api-alerting-notif-availability-slo
  - interval: 30s
    name: api-alerting-notif-availability-slo
    rules:
    - expr: sum by(code) (rate(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[5m]))
        / sum by(code) (rate(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[5m]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate5m
    - expr: sum by(code) (rate(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[30m]))
        / sum by(code) (rate(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[30m]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate30m
    - expr: sum by(code) (rate(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[1h]))
        / sum by(code) (rate(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[1h]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate1h
    - expr: sum by(code) (rate(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[2h]))
        / sum by(code) (rate(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[2h]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate2h
    - expr: sum by(code) (rate(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[6h]))
        / sum by(code) (rate(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[6h]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate6h
    - expr: sum by(code) (rate(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[1d]))
        / sum by(code) (rate(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[1d]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate1d
    - expr: sum by(code) (rate(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[4d]))
        / sum by(code) (rate(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}[4d]))
      labels:
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        slo: api-alerting-notif-availability-slo
      record: alertmanager_notifications_failed:burnrate4d
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and
          is burning too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate5m{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (14 * (1-0.995)) and alertmanager_notifications_failed:burnrate1h{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (14 * (1-0.995))
      for: 2m
      labels:
        long: 1h
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        severity: high
        short: 5m
        slo: api-alerting-notif-availability-slo
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and
          is burning too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate30m{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (7 * (1-0.995)) and alertmanager_notifications_failed:burnrate6h{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (7 * (1-0.995))
      for: 15m
      labels:
        long: 6h
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        severity: high
        short: 30m
        slo: api-alerting-notif-availability-slo
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and
          is burning too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate2h{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (2 * (1-0.995)) and alertmanager_notifications_failed:burnrate1d{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (2 * (1-0.995))
      for: 1h
      labels:
        long: 1d
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        severity: warning
        short: 2h
        slo: api-alerting-notif-availability-slo
    - alert: APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API Alertmanager failing to deliver alerts to upstream targets and
          is burning too much error budget to guarantee availability SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIAlertmanagerNotificationsAvailabilityErrorBudgetBurning
      expr: alertmanager_notifications_failed:burnrate6h{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (1 * (1-0.995)) and alertmanager_notifications_failed:burnrate4d{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager",slo="api-alerting-notif-availability-slo"}
        > (1 * (1-0.995))
      for: 3h
      labels:
        long: 4d
        namespace: observatorium-metrics-stage
        service: observatorium-alertmanager
        severity: warning
        short: 6h
        slo: api-alerting-notif-availability-slo
  - interval: 30s
    name: api-alerting-notif-availability-slo-generic
    rules:
    - expr: "0.995"
      labels:
        slo: api-alerting-notif-availability-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-alerting-notif-availability-slo
      record: pyrra_window
    - expr: 1 - sum(alertmanager_notifications_failed:increase4w{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}
        or vector(0)) / sum(alertmanager_notifications_failed:increase4w{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"})
      labels:
        slo: api-alerting-notif-availability-slo
      record: pyrra_availability
    - expr: sum(alertmanager_notifications_failed_total{code!~"^4..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"})
      labels:
        slo: api-alerting-notif-availability-slo
      record: pyrra_requests_total
    - expr: sum(alertmanager_notifications_failed_total{code=~"^5..$",namespace="observatorium-metrics-stage",service="observatorium-alertmanager"}
        or vector(0))
      labels:
        slo: api-alerting-notif-availability-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-write-latency-slo-increase
    rules:
    - expr: sum by(code) (increase(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[4w]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:increase4w
    - expr: sum by(code) (increase(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[4w]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        le: "5"
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        severity: high
        slo: api-metrics-write-latency-slo
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: absent(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"})
        == 1
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        le: "5"
        severity: high
        slo: api-metrics-write-latency-slo
  - interval: 30s
    name: api-metrics-write-latency-slo
    rules:
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[5m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[5m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[5m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate5m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[30m]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[30m])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[30m]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate30m
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[1h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate1h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[2h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[2h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[2h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate2h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[6h]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[6h])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[6h]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate6h
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[1d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[1d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate1d
    - expr: (sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[4d]))
        - sum(rate(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"}[4d])))
        / sum(rate(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"}[4d]))
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        slo: api-metrics-write-latency-slo
      record: http_request_duration_seconds:burnrate4d
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate5m{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (14 * (1-0.9)) and http_request_duration_seconds:burnrate1h{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (14 * (1-0.9))
      for: 2m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 1h
        severity: high
        short: 5m
        slo: api-metrics-write-latency-slo
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate30m{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (7 * (1-0.9)) and http_request_duration_seconds:burnrate6h{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (7 * (1-0.9))
      for: 15m
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 6h
        severity: high
        short: 30m
        slo: api-metrics-write-latency-slo
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate2h{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (2 * (1-0.9)) and http_request_duration_seconds:burnrate1d{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (2 * (1-0.9))
      for: 1h
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 1d
        severity: warning
        short: 2h
        slo: api-metrics-write-latency-slo
    - alert: APIMetricsWriteLatencyErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /receive handler is burning too much error budget to guarantee
          latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsWriteLatencyErrorBudgetBurning
      expr: http_request_duration_seconds:burnrate6h{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (1 * (1-0.9)) and http_request_duration_seconds:burnrate4d{group="metricsv1",handler="receive",job="observatorium-observatorium-api",slo="api-metrics-write-latency-slo"}
        > (1 * (1-0.9))
      for: 3h
      labels:
        group: metricsv1
        handler: receive
        job: observatorium-observatorium-api
        long: 4d
        severity: warning
        short: 6h
        slo: api-metrics-write-latency-slo
  - interval: 30s
    name: api-metrics-write-latency-slo-generic
    rules:
    - expr: "0.9"
      labels:
        slo: api-metrics-write-latency-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-metrics-write-latency-slo
      record: pyrra_window
    - expr: sum(http_request_duration_seconds:increase4w{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5",slo="api-metrics-write-latency-slo"}
        or vector(0)) / sum(http_request_duration_seconds:increase4w{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="",slo="api-metrics-write-latency-slo"})
      labels:
        slo: api-metrics-write-latency-slo
      record: pyrra_availability
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"})
      labels:
        slo: api-metrics-write-latency-slo
      record: pyrra_requests_total
    - expr: sum(http_request_duration_seconds_count{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api"})
        - sum(http_request_duration_seconds_bucket{code=~"^2..$",group="metricsv1",handler="receive",job="observatorium-observatorium-api",le="5"})
      labels:
        slo: api-metrics-write-latency-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-read-1M-latency-slo-increase
    rules:
    - expr: sum by(code) (increase(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4w]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:increase4w
    - expr: sum by(code) (increase(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4w]))
      labels:
        le: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 1M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency1MErrorBudgetBurning
      expr: absent(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
        == 1
      for: 2m
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        slo: api-metrics-read-1M-latency-slo
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 1M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency1MErrorBudgetBurning
      expr: absent(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
        == 1
      for: 2m
      labels:
        le: "10"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        slo: api-metrics-read-1M-latency-slo
  - interval: 30s
    name: api-metrics-read-1M-latency-slo
    rules:
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[5m]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[5m])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[5m]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:burnrate5m
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[30m]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[30m])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[30m]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:burnrate30m
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:burnrate1h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[2h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[2h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[2h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:burnrate2h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[6h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[6h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[6h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:burnrate6h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1d]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1d])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1d]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:burnrate1d
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4d]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4d])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4d]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-1M-latency-slo
      record: up_custom_query_duration_seconds:burnrate4d
    - alert: APIMetricsReadLatency1MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 1M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency1MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate5m{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (14 * (1-0.9)) and up_custom_query_duration_seconds:burnrate1h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (14 * (1-0.9))
      for: 2m
      labels:
        long: 1h
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        short: 5m
        slo: api-metrics-read-1M-latency-slo
    - alert: APIMetricsReadLatency1MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 1M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency1MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate30m{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (7 * (1-0.9)) and up_custom_query_duration_seconds:burnrate6h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (7 * (1-0.9))
      for: 15m
      labels:
        long: 6h
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        short: 30m
        slo: api-metrics-read-1M-latency-slo
    - alert: APIMetricsReadLatency1MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 1M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency1MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate2h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (2 * (1-0.9)) and up_custom_query_duration_seconds:burnrate1d{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (2 * (1-0.9))
      for: 1h
      labels:
        long: 1d
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: warning
        short: 2h
        slo: api-metrics-read-1M-latency-slo
    - alert: APIMetricsReadLatency1MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 1M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency1MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate6h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (1 * (1-0.9)) and up_custom_query_duration_seconds:burnrate4d{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        > (1 * (1-0.9))
      for: 3h
      labels:
        long: 4d
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: warning
        short: 6h
        slo: api-metrics-read-1M-latency-slo
  - interval: 30s
    name: api-metrics-read-1M-latency-slo-generic
    rules:
    - expr: "0.9"
      labels:
        slo: api-metrics-read-1M-latency-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-metrics-read-1M-latency-slo
      record: pyrra_window
    - expr: sum(up_custom_query_duration_seconds:increase4w{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"}
        or vector(0)) / sum(up_custom_query_duration_seconds:increase4w{code=~"^2..$",le="",namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-1M-latency-slo"})
      labels:
        slo: api-metrics-read-1M-latency-slo
      record: pyrra_availability
    - expr: sum(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
      labels:
        slo: api-metrics-read-1M-latency-slo
      record: pyrra_requests_total
    - expr: sum(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
        - sum(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="10",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
      labels:
        slo: api-metrics-read-1M-latency-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-read-10M-latency-slo-increase
    rules:
    - expr: sum by(code) (increase(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[4w]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:increase4w
    - expr: sum by(code) (increase(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[4w]))
      labels:
        le: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency10MErrorBudgetBurning
      expr: absent(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"})
        == 1
      for: 2m
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        severity: high
        slo: api-metrics-read-10M-latency-slo
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency10MErrorBudgetBurning
      expr: absent(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"})
        == 1
      for: 2m
      labels:
        le: "30"
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        severity: high
        slo: api-metrics-read-10M-latency-slo
  - interval: 30s
    name: api-metrics-read-10M-latency-slo
    rules:
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[5m]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[5m])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[5m]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:burnrate5m
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[30m]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[30m])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[30m]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:burnrate30m
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[1h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[1h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[1h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:burnrate1h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[2h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[2h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[2h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:burnrate2h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[6h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[6h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[6h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:burnrate6h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[1d]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[1d])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[1d]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:burnrate1d
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[4d]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[4d])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"}[4d]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        slo: api-metrics-read-10M-latency-slo
      record: up_custom_query_duration_seconds:burnrate4d
    - alert: APIMetricsReadLatency10MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency10MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate5m{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (14 * (1-0.9)) and up_custom_query_duration_seconds:burnrate1h{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (14 * (1-0.9))
      for: 2m
      labels:
        long: 1h
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        severity: high
        short: 5m
        slo: api-metrics-read-10M-latency-slo
    - alert: APIMetricsReadLatency10MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency10MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate30m{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (7 * (1-0.9)) and up_custom_query_duration_seconds:burnrate6h{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (7 * (1-0.9))
      for: 15m
      labels:
        long: 6h
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        severity: high
        short: 30m
        slo: api-metrics-read-10M-latency-slo
    - alert: APIMetricsReadLatency10MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency10MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate2h{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (2 * (1-0.9)) and up_custom_query_duration_seconds:burnrate1d{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (2 * (1-0.9))
      for: 1h
      labels:
        long: 1d
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        severity: warning
        short: 2h
        slo: api-metrics-read-10M-latency-slo
    - alert: APIMetricsReadLatency10MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency10MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate6h{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (1 * (1-0.9)) and up_custom_query_duration_seconds:burnrate4d{namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        > (1 * (1-0.9))
      for: 3h
      labels:
        long: 4d
        namespace: observatorium-stage
        query: query-path-sli-10M-samples
        severity: warning
        short: 6h
        slo: api-metrics-read-10M-latency-slo
  - interval: 30s
    name: api-metrics-read-10M-latency-slo-generic
    rules:
    - expr: "0.9"
      labels:
        slo: api-metrics-read-10M-latency-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-metrics-read-10M-latency-slo
      record: pyrra_window
    - expr: sum(up_custom_query_duration_seconds:increase4w{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"}
        or vector(0)) / sum(up_custom_query_duration_seconds:increase4w{code=~"^2..$",le="",namespace="observatorium-stage",query="query-path-sli-10M-samples",slo="api-metrics-read-10M-latency-slo"})
      labels:
        slo: api-metrics-read-10M-latency-slo
      record: pyrra_availability
    - expr: sum(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"})
      labels:
        slo: api-metrics-read-10M-latency-slo
      record: pyrra_requests_total
    - expr: sum(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-10M-samples"})
        - sum(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="30",namespace="observatorium-stage",query="query-path-sli-10M-samples"})
      labels:
        slo: api-metrics-read-10M-latency-slo
      record: pyrra_errors_total
  - interval: 2m30s
    name: api-metrics-read-100M-latency-slo-increase
    rules:
    - expr: sum by(code) (increase(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4w]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:increase4w
    - expr: sum by(code) (increase(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4w]))
      labels:
        le: "120"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:increase4w
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency100MErrorBudgetBurning
      expr: absent(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
        == 1
      for: 2m
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        slo: api-metrics-read-100M-latency-slo
    - alert: SLOMetricAbsent
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency100MErrorBudgetBurning
      expr: absent(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
        == 1
      for: 2m
      labels:
        le: "120"
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        slo: api-metrics-read-100M-latency-slo
  - interval: 30s
    name: api-metrics-read-100M-latency-slo
    rules:
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[5m]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[5m])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[5m]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:burnrate5m
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[30m]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[30m])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[30m]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:burnrate30m
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:burnrate1h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[2h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[2h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[2h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:burnrate2h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[6h]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[6h])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[6h]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:burnrate6h
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1d]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1d])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[1d]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:burnrate1d
    - expr: (sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4d]))
        - sum(rate(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4d])))
        / sum(rate(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"}[4d]))
      labels:
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        slo: api-metrics-read-100M-latency-slo
      record: up_custom_query_duration_seconds:burnrate4d
    - alert: APIMetricsReadLatency100MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency100MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate5m{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (14 * (1-0.9)) and up_custom_query_duration_seconds:burnrate1h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (14 * (1-0.9))
      for: 2m
      labels:
        long: 1h
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        short: 5m
        slo: api-metrics-read-100M-latency-slo
    - alert: APIMetricsReadLatency100MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency100MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate30m{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (7 * (1-0.9)) and up_custom_query_duration_seconds:burnrate6h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (7 * (1-0.9))
      for: 15m
      labels:
        long: 6h
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: high
        short: 30m
        slo: api-metrics-read-100M-latency-slo
    - alert: APIMetricsReadLatency100MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency100MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate2h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (2 * (1-0.9)) and up_custom_query_duration_seconds:burnrate1d{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (2 * (1-0.9))
      for: 1h
      labels:
        long: 1d
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: warning
        short: 2h
        slo: api-metrics-read-100M-latency-slo
    - alert: APIMetricsReadLatency100MErrorBudgetBurning
      annotations:
        dashboard: https://grafana.app-sre.devshift.net/d/080e53f245a15445bdf777ae0e66945d/telemeter-staging-slos?orgId=1&refresh=10s&var-datasource=app-sre-stage-01-prometheus&var-namespace={{$labels.namespace}}&var-job=All&var-pod=All&var-interval=5m
        message: API /query endpoint is burning too much error budget for 100M samples,
          to guarantee latency SLOs.
        runbook: https://github.com/rhobs/configuration/blob/main/docs/sop/observatorium.md#APIMetricsReadLatency100MErrorBudgetBurning
      expr: up_custom_query_duration_seconds:burnrate6h{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (1 * (1-0.9)) and up_custom_query_duration_seconds:burnrate4d{namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        > (1 * (1-0.9))
      for: 3h
      labels:
        long: 4d
        namespace: observatorium-stage
        query: query-path-sli-1M-samples
        severity: warning
        short: 6h
        slo: api-metrics-read-100M-latency-slo
  - interval: 30s
    name: api-metrics-read-100M-latency-slo-generic
    rules:
    - expr: "0.9"
      labels:
        slo: api-metrics-read-100M-latency-slo
      record: pyrra_objective
    - expr: 2419200
      labels:
        slo: api-metrics-read-100M-latency-slo
      record: pyrra_window
    - expr: sum(up_custom_query_duration_seconds:increase4w{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"}
        or vector(0)) / sum(up_custom_query_duration_seconds:increase4w{code=~"^2..$",le="",namespace="observatorium-stage",query="query-path-sli-1M-samples",slo="api-metrics-read-100M-latency-slo"})
      labels:
        slo: api-metrics-read-100M-latency-slo
      record: pyrra_availability
    - expr: sum(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
      labels:
        slo: api-metrics-read-100M-latency-slo
      record: pyrra_requests_total
    - expr: sum(up_custom_query_duration_seconds_count{code=~"^2..$",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
        - sum(up_custom_query_duration_seconds_bucket{code=~"^2..$",le="120",namespace="observatorium-stage",query="query-path-sli-1M-samples"})
      labels:
        slo: api-metrics-read-100M-latency-slo
      record: pyrra_errors_total
